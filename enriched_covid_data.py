# -*- coding: utf-8 -*-
"""enriched_covid_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HrnQl936TyRxKxP5ZStk_7V_sv3qJ5_w

# Data review and processing:
Objective:
1. View basic data information
2. Calculate daily new confirmed and death cases and optimize data
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, year, month, avg, sum as _sum, expr, lit
import requests

#  Download JSON from GitHub
url = "https://raw.githubusercontent.com/Aless13260/covid-pipeline/main/sample_data.json"
local_path = "/tmp/sample_data.json"
with open(local_path, "w") as f:
    f.write(requests.get(url).text)
df=pd.read_json(url)
# mian info check
print(df.head(10))
print(f"Total rows: {len(df)}")
print(f"Total rows with missing value:\n{df.isna().sum()}")

#unique values check

# for country
countries = df["country"].dropna().unique().tolist()
countries.sort()
print(f"Total unique countries: {len(countries)}")
print("Countries are:", countries[:])

#for state
states=df["state"].dropna().unique().tolist()
states.sort()
df["state"].unique()
print(f"Total unique sates: {len(states)}")
print("States are:", states[:])

"""**Observatone1:**
According to the results of the missing value check, the national data is basically unavailable, but most of the state data are missing. In this case , for the subsequent analysis, the geographical location analysis will mainly focus on the country.

**Observatone2:**
Death data is more lacking than confirmed data, and data sources may not be comprehensive enough for death statistics.Similarly, some of the confirmed data is missing
"""

# Start Spark session
spark = SparkSession.builder.appName("CovidDataPipeline").getOrCreate()

#  Load raw JSON from file
df_raw =df

#  Ensure correct types & date parsing
df_spark = df_raw.withColumn("date", to_date(col("date"), "yyyy-MM-dd"))

# Define window by country, state, date
window_spec = Window.partitionBy("country", "state").orderBy("date")

#  Add previous confirmed & death columns
df_spark = (
    df_spark
    .withColumn("prev_confirmed", lag("confirmed").over(window_spec))
    .withColumn("prev_deaths", lag("deaths").over(window_spec))
)

# Calculate daily new values
df_spark = (
    df_spark
    .withColumn("daily_new_cases", (col("confirmed") - col("prev_confirmed")))
    .withColumn("daily_new_deaths", (col("deaths") - col("prev_deaths")))
)

# Replace nulls with 0
df_spark = (
    df_spark
    .withColumn("daily_new_cases", when(col("daily_new_cases").isNull(), 0).otherwise(col("daily_new_cases")))
    .withColumn("daily_new_deaths", when(col("daily_new_deaths").isNull(), 0).otherwise(col("daily_new_deaths")))
)

# Show result
df_spark.select("date", "country", "state", "confirmed", "daily_new_cases", "deaths", "daily_new_deaths").show(10)

# Save 
df_spark.write.json("enriched_covid_data_spark.json")

